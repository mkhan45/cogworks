{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding MyGrad\n",
    "> CogWorks 2018 (Petar Griggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell!\n",
    "import mygrad as mg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a list of numbers ranging from 0 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(range(51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a [`Tensor`](https://mygrad.readthedocs.io/en/latest/tensor.html)-instance by passing it some iterable of numbers. This includes lists, tuples, or NumPy arrays. Any array-like sequence of numbers will suffice. When we do so, the `Tensor.__init__()` simply converts this iterable to a NumPy `ndarray`. We can inspect a Tensor's underlying NumPy array by calling `<tensor>.data`. \n",
    "\n",
    "The takeaway is that: whatever data you pass in when creating a tensor will be stored as a NumPy array. You can access that underlying data **but should never unwittingly modify that NumPy array directly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass `ls` to `mg.Tensor` and check the shape and contents of the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mg.Tensor(ls)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `data` attribute of the Tensor that you created and see that it is the Tensor's underlying NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try mutating the underlying NumPy array using an augmented assignment (e.g. `x += 2`), and see that this changes the Tensor correspondingly. This will come in handy when we are updating parameters via gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "        21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "        38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data += 2\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try constructing Tensors with other iterables below:\n",
    " - a 3D NumPy array\n",
    " - a 2D structure of nested lists\n",
    " - a single number (the only permissable non-iterable object that can be passed to `Tensor.__init__`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n",
       "         [ 10,  11,  12,  13,  14,  15,  16,  17,  18,  19],\n",
       "         [ 20,  21,  22,  23,  24,  25,  26,  27,  28,  29],\n",
       "         [ 30,  31,  32,  33,  34,  35,  36,  37,  38,  39],\n",
       "         [ 40,  41,  42,  43,  44,  45,  46,  47,  48,  49],\n",
       "         [ 50,  51,  52,  53,  54,  55,  56,  57,  58,  59],\n",
       "         [ 60,  61,  62,  63,  64,  65,  66,  67,  68,  69],\n",
       "         [ 70,  71,  72,  73,  74,  75,  76,  77,  78,  79],\n",
       "         [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89],\n",
       "         [ 90,  91,  92,  93,  94,  95,  96,  97,  98,  99]],\n",
       " \n",
       "        [[100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       "         [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n",
       "         [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n",
       "         [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n",
       "         [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "         [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "         [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n",
       "         [170, 171, 172, 173, 174, 175, 176, 177, 178, 179],\n",
       "         [180, 181, 182, 183, 184, 185, 186, 187, 188, 189],\n",
       "         [190, 191, 192, 193, 194, 195, 196, 197, 198, 199]],\n",
       " \n",
       "        [[200, 201, 202, 203, 204, 205, 206, 207, 208, 209],\n",
       "         [210, 211, 212, 213, 214, 215, 216, 217, 218, 219],\n",
       "         [220, 221, 222, 223, 224, 225, 226, 227, 228, 229],\n",
       "         [230, 231, 232, 233, 234, 235, 236, 237, 238, 239],\n",
       "         [240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n",
       "         [250, 251, 252, 253, 254, 255, 256, 257, 258, 259],\n",
       "         [260, 261, 262, 263, 264, 265, 266, 267, 268, 269],\n",
       "         [270, 271, 272, 273, 274, 275, 276, 277, 278, 279],\n",
       "         [280, 281, 282, 283, 284, 285, 286, 287, 288, 289],\n",
       "         [290, 291, 292, 293, 294, 295, 296, 297, 298, 299]],\n",
       " \n",
       "        [[300, 301, 302, 303, 304, 305, 306, 307, 308, 309],\n",
       "         [310, 311, 312, 313, 314, 315, 316, 317, 318, 319],\n",
       "         [320, 321, 322, 323, 324, 325, 326, 327, 328, 329],\n",
       "         [330, 331, 332, 333, 334, 335, 336, 337, 338, 339],\n",
       "         [340, 341, 342, 343, 344, 345, 346, 347, 348, 349],\n",
       "         [350, 351, 352, 353, 354, 355, 356, 357, 358, 359],\n",
       "         [360, 361, 362, 363, 364, 365, 366, 367, 368, 369],\n",
       "         [370, 371, 372, 373, 374, 375, 376, 377, 378, 379],\n",
       "         [380, 381, 382, 383, 384, 385, 386, 387, 388, 389],\n",
       "         [390, 391, 392, 393, 394, 395, 396, 397, 398, 399]],\n",
       " \n",
       "        [[400, 401, 402, 403, 404, 405, 406, 407, 408, 409],\n",
       "         [410, 411, 412, 413, 414, 415, 416, 417, 418, 419],\n",
       "         [420, 421, 422, 423, 424, 425, 426, 427, 428, 429],\n",
       "         [430, 431, 432, 433, 434, 435, 436, 437, 438, 439],\n",
       "         [440, 441, 442, 443, 444, 445, 446, 447, 448, 449],\n",
       "         [450, 451, 452, 453, 454, 455, 456, 457, 458, 459],\n",
       "         [460, 461, 462, 463, 464, 465, 466, 467, 468, 469],\n",
       "         [470, 471, 472, 473, 474, 475, 476, 477, 478, 479],\n",
       "         [480, 481, 482, 483, 484, 485, 486, 487, 488, 489],\n",
       "         [490, 491, 492, 493, 494, 495, 496, 497, 498, 499]],\n",
       " \n",
       "        [[500, 501, 502, 503, 504, 505, 506, 507, 508, 509],\n",
       "         [510, 511, 512, 513, 514, 515, 516, 517, 518, 519],\n",
       "         [520, 521, 522, 523, 524, 525, 526, 527, 528, 529],\n",
       "         [530, 531, 532, 533, 534, 535, 536, 537, 538, 539],\n",
       "         [540, 541, 542, 543, 544, 545, 546, 547, 548, 549],\n",
       "         [550, 551, 552, 553, 554, 555, 556, 557, 558, 559],\n",
       "         [560, 561, 562, 563, 564, 565, 566, 567, 568, 569],\n",
       "         [570, 571, 572, 573, 574, 575, 576, 577, 578, 579],\n",
       "         [580, 581, 582, 583, 584, 585, 586, 587, 588, 589],\n",
       "         [590, 591, 592, 593, 594, 595, 596, 597, 598, 599]],\n",
       " \n",
       "        [[600, 601, 602, 603, 604, 605, 606, 607, 608, 609],\n",
       "         [610, 611, 612, 613, 614, 615, 616, 617, 618, 619],\n",
       "         [620, 621, 622, 623, 624, 625, 626, 627, 628, 629],\n",
       "         [630, 631, 632, 633, 634, 635, 636, 637, 638, 639],\n",
       "         [640, 641, 642, 643, 644, 645, 646, 647, 648, 649],\n",
       "         [650, 651, 652, 653, 654, 655, 656, 657, 658, 659],\n",
       "         [660, 661, 662, 663, 664, 665, 666, 667, 668, 669],\n",
       "         [670, 671, 672, 673, 674, 675, 676, 677, 678, 679],\n",
       "         [680, 681, 682, 683, 684, 685, 686, 687, 688, 689],\n",
       "         [690, 691, 692, 693, 694, 695, 696, 697, 698, 699]],\n",
       " \n",
       "        [[700, 701, 702, 703, 704, 705, 706, 707, 708, 709],\n",
       "         [710, 711, 712, 713, 714, 715, 716, 717, 718, 719],\n",
       "         [720, 721, 722, 723, 724, 725, 726, 727, 728, 729],\n",
       "         [730, 731, 732, 733, 734, 735, 736, 737, 738, 739],\n",
       "         [740, 741, 742, 743, 744, 745, 746, 747, 748, 749],\n",
       "         [750, 751, 752, 753, 754, 755, 756, 757, 758, 759],\n",
       "         [760, 761, 762, 763, 764, 765, 766, 767, 768, 769],\n",
       "         [770, 771, 772, 773, 774, 775, 776, 777, 778, 779],\n",
       "         [780, 781, 782, 783, 784, 785, 786, 787, 788, 789],\n",
       "         [790, 791, 792, 793, 794, 795, 796, 797, 798, 799]],\n",
       " \n",
       "        [[800, 801, 802, 803, 804, 805, 806, 807, 808, 809],\n",
       "         [810, 811, 812, 813, 814, 815, 816, 817, 818, 819],\n",
       "         [820, 821, 822, 823, 824, 825, 826, 827, 828, 829],\n",
       "         [830, 831, 832, 833, 834, 835, 836, 837, 838, 839],\n",
       "         [840, 841, 842, 843, 844, 845, 846, 847, 848, 849],\n",
       "         [850, 851, 852, 853, 854, 855, 856, 857, 858, 859],\n",
       "         [860, 861, 862, 863, 864, 865, 866, 867, 868, 869],\n",
       "         [870, 871, 872, 873, 874, 875, 876, 877, 878, 879],\n",
       "         [880, 881, 882, 883, 884, 885, 886, 887, 888, 889],\n",
       "         [890, 891, 892, 893, 894, 895, 896, 897, 898, 899]],\n",
       " \n",
       "        [[900, 901, 902, 903, 904, 905, 906, 907, 908, 909],\n",
       "         [910, 911, 912, 913, 914, 915, 916, 917, 918, 919],\n",
       "         [920, 921, 922, 923, 924, 925, 926, 927, 928, 929],\n",
       "         [930, 931, 932, 933, 934, 935, 936, 937, 938, 939],\n",
       "         [940, 941, 942, 943, 944, 945, 946, 947, 948, 949],\n",
       "         [950, 951, 952, 953, 954, 955, 956, 957, 958, 959],\n",
       "         [960, 961, 962, 963, 964, 965, 966, 967, 968, 969],\n",
       "         [970, 971, 972, 973, 974, 975, 976, 977, 978, 979],\n",
       "         [980, 981, 982, 983, 984, 985, 986, 987, 988, 989],\n",
       "         [990, 991, 992, 993, 994, 995, 996, 997, 998, 999]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mg.arange(1000).reshape(10, 10, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyGrad also has many convenient tensor-creation functions that mimic those of NumPy, such as [`arange`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.arange.html#mygrad.arange) and [`linspace`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.linspace.html#mygrad.linspace).\n",
    "\n",
    "Execute `help(mg.tensor_creation.funcs)` to view these tensor-creation functions and their documentation, or see the [`mygrad` docs](https://mygrad.readthedocs.io/en/latest/tensor_creation.html). Notice that each of these functions accepts an optional `constant` argument. What do you suppose the purpose of this is?\n",
    "\n",
    "Use `mg.linspace` to create a constant `Tensor` consisting of 100 points sampled on $[-1, 1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mygrad.tensor_creation.funcs in mygrad.tensor_creation:\n",
      "\n",
      "NAME\n",
      "    mygrad.tensor_creation.funcs\n",
      "\n",
      "FUNCTIONS\n",
      "    arange(stop, start=0, step=1, dtype=None, constant=False)\n",
      "        Return a Tensor with evenly-spaced values within a given interval.\n",
      "        \n",
      "        Values are generated within [start, stop). Note that for non-integer steps, results may be\n",
      "        inconsistent; you are better off using `linspace` instead.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : Real, optional, default=0\n",
      "            The start of the interval, inclusive.\n",
      "        \n",
      "        stop : Real\n",
      "            The end of the interval, exclusive.\n",
      "        \n",
      "        step : Real, optional (default=1)\n",
      "            The spacing between successive values.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            The data type of the output Tensor, or None to infer from the inputs.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of evenly-spaced values in [start, end).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.arange(3)\n",
      "        Tensor([0, 1, 2])\n",
      "        >>> mg.arange(3.0, constant=True)\n",
      "        Tensor([ 0.,  1.,  2.])  # resulting tensor will not back-propagate a gradient\n",
      "        >>> mg.arange(3,7)\n",
      "        Tensor([3, 4, 5, 6])\n",
      "        >>> mg.arange(3,7,2)\n",
      "        Tensor([3, 5])\n",
      "    \n",
      "    empty(shape, dtype=<class 'numpy.float32'>, constant=False)\n",
      "        Return a new Tensor of the given shape and type, without initializing entries.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : Union[int, Tuple[int]]\n",
      "            The shape of the empty array.\n",
      "        \n",
      "        dtype : data-type, optional (default=numpy.float32)\n",
      "            The data type of the output Tensor.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A tensor of uninitialized data of the given shape and dtype.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty tensor with shape and type of input.\n",
      "        ones : Return a new tensor setting values to one.\n",
      "        zeros : Return a new tensor setting values to zero.\n",
      "        full : Return a new tensor of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `empty`, unlike `zeros`, does not set the array values to zero,\n",
      "        and may therefore be marginally faster.  On the other hand, it requires\n",
      "        the user to manually set all the values in the array, and should be\n",
      "        used with caution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.empty([2, 2], constant=True)\n",
      "        Tensor([[ -9.74499359e+001,   6.69583040e-309],\n",
      "                [  2.13182611e-314,   3.06959433e-309]])         #random\n",
      "        \n",
      "        >>> mg.empty([2, 2], dtype=int)\n",
      "        Tensor([[-1073741821, -1067949133],\n",
      "                [  496041986,    19249760]])                     #random\n",
      "    \n",
      "    empty_like(other, dtype=None, constant=False)\n",
      "        Return a new Tensor of the same shape and type as the given array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Union[Tensor, ArrayLike]\n",
      "            The Tensor or array whose shape and datatype should be mirrored.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            Override the data type of the returned Tensor with this value, or None to not override.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A tensor of uninitialized data whose shape and type match `other`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> x = mg.arange(4).reshape\n",
      "        >>> mg.empty(x, constant=True)\n",
      "        Tensor([[ -9.74499359e+001,   6.69583040e-309],\n",
      "                [  2.13182611e-314,   3.06959433e-309]])         #random\n",
      "        \n",
      "        >>> mg.empty(x, dtype=int)\n",
      "        Tensor([[-1073741821, -1067949133],\n",
      "                [  496041986,    19249760]])                     #random\n",
      "    \n",
      "    eye(rows, cols=None, diag_idx=0, dtype=<class 'numpy.float32'>, constant=False)\n",
      "        Return a 2D Tensor with ones on the diagonal and zeros elsewhere.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rows : int\n",
      "            The number of rows in the output Tensor.\n",
      "        \n",
      "        cols : int, optional (default=None)\n",
      "            The number of columns in the output, or None to match `rows`.\n",
      "        \n",
      "        diag_idx : int, optional (default=0)\n",
      "            The index of the diagonal. 0 is the main diagonal; a positive value is the upper\n",
      "            diagonal, while a negative value refers to the lower diagonal.\n",
      "        \n",
      "        dtype : data-type, optional (default=numpy.float32)\n",
      "            The data type of the output Tensor.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A tensor whose elements are 0, except for the :math:`k`-th diagonal, whose values are 1.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.eye(2, dtype=int)\n",
      "        Tensor([[1, 0],\n",
      "                [0, 1]])\n",
      "        >>> mg.eye(3, k=1)\n",
      "        Tensor([[ 0.,  1.,  0.],\n",
      "                [ 0.,  0.,  1.],\n",
      "                [ 0.,  0.,  0.]])\n",
      "    \n",
      "    full(shape, fill_value, dtype=None, constant=False)\n",
      "        Return a Tensor of the given shape and type, filled with `fill_value`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : Union[int, Tuple[int]]\n",
      "            The shape of the output Tensor.\n",
      "        \n",
      "        fill_value : Real\n",
      "            The value with which to fill the output Tensor.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            The data type of the output Tensor, or None to match `fill_value`..\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "                does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of `fill_value` with the given shape and dtype.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.full((2, 2), 33)\n",
      "        Tensor([[ 33,  33],\n",
      "                [ 33,  33]])\n",
      "        \n",
      "        >>> mg.full((2, 2), 10)\n",
      "        Tensor([[10, 10],\n",
      "                [10, 10]])\n",
      "    \n",
      "    full_like(other, fill_value, dtype=None, constant=False)\n",
      "        Return a Tensor of the same shape and type as the given, filled with `fill_value`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Union[Tensor, ArrayLike]\n",
      "            The Tensor or array whose shape and datatype should be mirrored.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            Override the data type of the returned Tensor with this value, or None to not override.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of `fill_value` whose shape and data type match `other`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> x = mg.arange(6, dtype=int)\n",
      "        >>> mg.full_like(x, 1)\n",
      "        Tensor([1, 1, 1, 1, 1, 1])\n",
      "        >>> mg.full_like(x, 0.1)\n",
      "        Tensor([0, 0, 0, 0, 0, 0])\n",
      "        >>> mg.full_like(x, 0.1, dtype=np.double)\n",
      "        Tensor([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1])\n",
      "        >>> mg.full_like(x, np.nan, dtype=np.double)\n",
      "        Tensor([ nan,  nan,  nan,  nan,  nan,  nan])\n",
      "        \n",
      "        >>> y = mg.arange(6, dtype=np.double)\n",
      "        >>> mg.full_like(y, 0.1)\n",
      "        Tensor([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1])\n",
      "    \n",
      "    geomspace(start, stop, num=50, include_endpoint=True, dtype=None, constant=False)\n",
      "        Return a Tensor with evenly-spaced values in a geometric progression.\n",
      "        \n",
      "        Each output sample is a constant multiple of the previous output.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : Real\n",
      "            The starting value of the output.\n",
      "        \n",
      "        stop : Real\n",
      "            The ending value of the sequence, inclusive unless `include_endpoint` is false.\n",
      "        \n",
      "        num : int, optional (default=50)\n",
      "            The number of values to generate. Must be non-negative.\n",
      "        \n",
      "        include_endpoint : bool, optional (default=True)\n",
      "            Whether to include the endpoint in the Tensor. Note that if False, the step size changes\n",
      "            to accommodate the sequence excluding the endpoint.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            The data type of the output Tensor, or None to infer from the inputs.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of `num` samples, evenly-spaced in a geometric progression.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        logspace : Similar to geomspace, but with endpoints specified using log\n",
      "                   and base.\n",
      "        linspace : Similar to geomspace, but with arithmetic instead of geometric\n",
      "                   progression.\n",
      "        arange : Similar to linspace, with the step size specified instead of the\n",
      "                 number of samples.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> import numpy as np\n",
      "        >>> mg.geomspace(1, 1000, num=4)\n",
      "        Tensor([    1.,    10.,   100.,  1000.])\n",
      "        >>> mg.geomspace(1, 1000, num=3, endpoint=False)\n",
      "        Tensor([   1.,   10.,  100.])\n",
      "        >>> mg.geomspace(1, 1000, num=4, endpoint=False)\n",
      "        Tensor([   1.        ,    5.62341325,   31.6227766 ,  177.827941  ])\n",
      "        >>> mg.geomspace(1, 256, num=9)\n",
      "        Tensor([   1.,    2.,    4.,    8.,   16.,   32.,   64.,  128.,  256.])\n",
      "        \n",
      "        Note that the above may not produce exact integers:\n",
      "        \n",
      "        >>> mg.geomspace(1, 256, num=9, dtype=int)\n",
      "        Tensor([  1,   2,   4,   7,  16,  32,  63, 127, 256])\n",
      "        >>> np.around(mg.geomspace(1, 256, num=9).data).astype(int)\n",
      "        array([  1,   2,   4,   8,  16,  32,  64, 128, 256])\n",
      "        \n",
      "        Negative, decreasing, and complex inputs are allowed:\n",
      "        \n",
      "        >>> mg.geomspace(1000, 1, num=4)\n",
      "        Tensor([ 1000.,   100.,    10.,     1.])\n",
      "        >>> mg.geomspace(-1000, -1, num=4)\n",
      "        Tensor([-1000.,  -100.,   -10.,    -1.])\n",
      "    \n",
      "    identity(n, dtype=<class 'numpy.float32'>, constant=False)\n",
      "        Return the identity Tensor; a square Tensor with 1s on the main diagonal and 0s elsewhere.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            The number of rows and columns in the output Tensor.\n",
      "        \n",
      "        dtype : data-type, optional (default=numpy.float32)\n",
      "            The data type of the output Tensor.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A square Tensor whose main diagonal is 1 and all other elements are 0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> importy mygrad as mg\n",
      "        >>> mg.identity(3)\n",
      "        Tensor([[ 1.,  0.,  0.],\n",
      "                [ 0.,  1.,  0.],\n",
      "                [ 0.,  0.,  1.]])\n",
      "    \n",
      "    linspace(start, stop, num=50, include_endpoint=True, dtype=None, constant=False)\n",
      "        Return a Tensor with evenly-spaced numbers over a specified interval.\n",
      "        \n",
      "        Values are generated within [start, stop], with the endpoint optionally excluded.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : Real\n",
      "            The starting value of the sequence, inclusive.\n",
      "        \n",
      "        stop : Real\n",
      "            The ending value of the sequence, inclusive unless `include_endpoint` is False.\n",
      "        \n",
      "        num : int, optional (default=50)\n",
      "            The number of values to generate. Must be non-negative.\n",
      "        \n",
      "        include_endpoint : bool, optional (default=True)\n",
      "            Whether to include the endpoint in the Tensor. Note that if False, the step size changes\n",
      "            to accommodate the sequence excluding the endpoint.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            The data type of the output Tensor, or None to infer from the inputs.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of `num` evenly-spaced values in [start, stop] or [start, stop), depending on\n",
      "            `include_endpoint`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        arange : Similar to `linspace`, but uses a step size (instead of the\n",
      "                 number of samples).\n",
      "        logspace : Samples uniformly distributed in log space.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.linspace(2.0, 3.0, num=5)\n",
      "        Tensor([ 2.  ,  2.25,  2.5 ,  2.75,  3.  ])\n",
      "        >>> mg.linspace(2.0, 3.0, num=5, endpoint=False)\n",
      "        Tensor([ 2. ,  2.2,  2.4,  2.6,  2.8])\n",
      "    \n",
      "    logspace(start, stop, num=50, include_endpoint=True, base=10, dtype=None, constant=False)\n",
      "        Return a Tensor with evenly-spaced numbers over a specified interval on a log scale.\n",
      "        \n",
      "        In linear space, values are generated within [base**start, base**stop], with the endpoint\n",
      "        optionally excluded.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : Real\n",
      "            The starting value of the sequence, inclusive; start at `base ** start`.\n",
      "        \n",
      "        stop : Real\n",
      "            The ending value of the sequence, inclusive unless `include_endpoint` is False; end at\n",
      "            `base ** stop`.\n",
      "        \n",
      "        num : int, optional (default=50)\n",
      "            The number of values to generate. Must be non-negative.\n",
      "        \n",
      "        include_endpoint : bool, optional (default=True)\n",
      "            Whether to include the endpoint in the Tensor. Note that if False, the step size changes\n",
      "            to accommodate the sequence excluding the endpoint.\n",
      "        \n",
      "        base : Real, optional (default=10)\n",
      "            The base of the log space.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            The data type of the output Tensor, or None to infer from the inputs.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "            does not back-propagate a gradient)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        arange : Similar to linspace, with the step size specified instead of the\n",
      "                 number of samples. Note that, when used with a float endpoint, the\n",
      "                 endpoint may or may not be included.\n",
      "        linspace : Similar to logspace, but with the samples uniformly distributed\n",
      "                   in linear space, instead of log space.\n",
      "        geomspace : Similar to logspace, but with endpoints specified directly.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.logspace(2.0, 3.0, num=4)\n",
      "        Tensor([  100.        ,   215.443469  ,   464.15888336,  1000.        ])\n",
      "        >>> mg.logspace(2.0, 3.0, num=4, endpoint=False)\n",
      "        Tensor([ 100.        ,  177.827941  ,  316.22776602,  562.34132519])\n",
      "        >>> mg.logspace(2.0, 3.0, num=4, base=2.0)\n",
      "        Tensor([ 4.        ,  5.0396842 ,  6.34960421,  8.        ])\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of `num` evenly-spaced values in the log interval [base**start, base**stop].\n",
      "    \n",
      "    ones(shape, dtype=<class 'numpy.float32'>, constant=False)\n",
      "        Return a Tensor of the given shape and type, filled with ones.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : Union[int, Tuple[int]]\n",
      "            The shape of the output Tensor.\n",
      "        \n",
      "        dtype : data-type, optional (default=numpy.float32)\n",
      "            The data type of the output Tensor.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "                does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of ones with the given shape and data type.\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ones_like : Return an tensor of ones with shape and type of input.\n",
      "        empty : Return a new uninitialized tensor.\n",
      "        zeros : Return a new tensor setting values to zero.\n",
      "        full : Return a new tensor of given shape filled with value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.ones(5)\n",
      "        Tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "        \n",
      "        >>> mg.ones((5,), dtype=int)\n",
      "        Tensor([1, 1, 1, 1, 1])\n",
      "        \n",
      "        >>> mg.ones((2, 1))\n",
      "        Tensor([[ 1.],\n",
      "               [ 1.]])\n",
      "        \n",
      "        >>> mg.ones((2, 2))\n",
      "        Tensor([[ 1.,  1.],\n",
      "                [ 1.,  1.]])\n",
      "    \n",
      "    ones_like(other, dtype=None, constant=False)\n",
      "        Return a Tensor of the same shape and type as the given, filled with ones.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        other : array_like\n",
      "            The Tensor or array whose shape and datatype should be mirrored.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            Override the data type of the returned Tensor with this value, or None to not override.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "                does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of ones whose shape and data type match `other`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> x = mg.arange(6).reshape((2, 3))\n",
      "        >>> x\n",
      "        Tensor([[0, 1, 2],\n",
      "                [3, 4, 5]])\n",
      "        \n",
      "        >>> mg.ones_like(x)\n",
      "        Tensor([[1, 1, 1],\n",
      "                [1, 1, 1]])\n",
      "        \n",
      "        >>> y = mg.arange(3, dtype=float)\n",
      "        >>> y\n",
      "        Tensor([ 0.,  1.,  2.])\n",
      "        \n",
      "        >>> mg.ones_like(y)\n",
      "        Tensor([ 1.,  1.,  1.])\n",
      "    \n",
      "    zeros(shape, dtype=<class 'numpy.float32'>, constant=False)\n",
      "        Return a Tensor of the given shape and type, filled with zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : Union[int, Tuple[int]]\n",
      "            The shape of the output Tensor.\n",
      "        \n",
      "        dtype : data-type, optional (default=numpy.float32)\n",
      "            The data type of the output Tensor.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "                does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of zeros with the given shape and data type.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ones_like : Return an tensor of ones with shape and type of input.\n",
      "        empty : Return a new uninitialized tensor.\n",
      "        ones : Return a new tensor setting values to one.\n",
      "        full : Return a new tensor of given shape filled with value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> mg.zeros(5)\n",
      "        Tensor([ 0.,  0.,  0.,  0.,  0.])\n",
      "        \n",
      "        >>> mg.zeros((5,), dtype=int, constant=True) # tensor will not back-propagate a gradient\n",
      "        Tensor([0, 0, 0, 0, 0])\n",
      "        \n",
      "        >>> mg.zeros((2, 1))\n",
      "        Tensor([[ 0.],\n",
      "                [ 0.]])\n",
      "        \n",
      "        >>> mg.zeros((2, 2))\n",
      "        Tensor([[ 0.,  0.],\n",
      "                [ 0.,  0.]])\n",
      "    \n",
      "    zeros_like(other, dtype=None, constant=False)\n",
      "        Return a Tensor of the same shape and type as the given, filled with zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Union[Tensor, ArrayLike]\n",
      "            The Tensor or array whose shape and datatype should be mirrored.\n",
      "        \n",
      "        dtype : data-type, optional (default=None)\n",
      "            Override the data type of the returned Tensor with this value, or None to not override.\n",
      "        \n",
      "        constant : bool, optional (default=False)\n",
      "            If ``True``, the returned tensor is a constant (it\n",
      "                does not back-propagate a gradient)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Tensor\n",
      "            A Tensor of zeros whose shape and data type match `other`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty tensor with shape and type of input.\n",
      "        ones_like : Return an tensor of ones with shape and type of input.\n",
      "        full_like : Return a new tensor with shape of input filled with value.\n",
      "        zeros : Return a new tensor setting values to zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import mygrad as mg\n",
      "        >>> x = mg.arange(6).reshape((2, 3))\n",
      "        >>> x\n",
      "        Tensor([[0, 1, 2],\n",
      "                [3, 4, 5]])\n",
      "        \n",
      "        >>> mg.zeros_like(x, constant=True)  # tensor will not back-propagate a gradient\n",
      "        Tensor([[0, 0, 0],\n",
      "                [0, 0, 0]])\n",
      "        \n",
      "        >>> y = mg.arange(3, dtype=float)\n",
      "        >>> y\n",
      "        Tensor([ 0.,  1.,  2.])\n",
      "        \n",
      "        >>> mg.zeros_like(y)\n",
      "        Tensor([ 0.,  0.,  0.])\n",
      "\n",
      "DATA\n",
      "    __all__ = ['empty', 'empty_like', 'eye', 'identity', 'ones', 'ones_lik...\n",
      "\n",
      "FILE\n",
      "    /home/fish/.local/lib/python3.7/site-packages/mygrad/tensor_creation/funcs.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mg.tensor_creation.funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tensor Attributes\n",
    "Tensors also store a number of other attributes: `scalar_only`, `grad`, `creator`, and `constant`. You can inspect the Tensor docstring for a description of some of these or read the docs [here](https://mygrad.readthedocs.io/en/latest/tensor.html#documentation-for-mygrad-tensor), but we will also describe them more in detail in due time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we perform mathematical operations using Tensors, we build up a computational graph whose nodes are Tensors and Operations. The edges connect tensor-nodes with operation-nodes.\n",
    "\n",
    "Edges connect input-tensors to operations, and operations to output-tensors. Each new operation we perform adds new nodes and edges to our computational graph. Below is an example of a computational graph that you may remember from the online course, adapted slightly for MyGrad.\n",
    "\n",
    "The following series of operations:\n",
    "\n",
    "```python\n",
    "x = mg.Tensor(10)\n",
    "y = np.array(15)\n",
    "out1 = x + y\n",
    "z = 2\n",
    "out2 = out1 * z\n",
    "```\n",
    "\n",
    "creates the computational graph:\n",
    "\n",
    "![comp_graph](pics/comp_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The creator attribute\n",
    "Each tensor has a [`creator`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.Tensor.creator.html#mygrad.Tensor.creator) attribute. This stores a reference to the Operation-instance that created that Tensor. `creator` will be `None` for any Tensor that was initialized directly, and not produced by a mathematical operation.\n",
    "\n",
    "#### The variables attribute\n",
    "Similarly, every Operation has a `variables` attribute, a tuple which stores all of the Tensors that acted as its inputs.\n",
    "\n",
    "Together, `Tensor.creator` and `Operation.variables` are the two attributes required for defining the structure of any computational graph.\n",
    "\n",
    "#### Tracing through a computational graph\n",
    "Create the computational graph detailed above and inspect the creator-attribute of `out2`. Confirm that this is an instance of the `Multiplication` operation. Next, check the variables-attribute of that operation; it should store a tuple containing `out1` and `z`. Continue on until you've traced backward through the computational graph, back to `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-3e4e9d57395b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-3e4e9d57395b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    no u\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "no u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyGrad must construct this computational graph so that it may use back-propagation to compute derivatives. More on this later. In the meantime it is critical to know that MyGrad is constructing these computational graphs \"under the hood\" whenever we perform mathematical operations on its Tensors.\n",
    "\n",
    "You may be surprised to see that a NumPy-array was able to be used in your computational graph - MyGrad so closely mirrors NumPy that it is natural to permit the use of NumPy-arrays in its computational graphs. The key detail here is that *NumPy-arrays will always be treated as constants in computational graphs*. That is, if we perform back-propagation on this graph, no derivative will be computed for `y`, since it is a NumPy-array. This will prove to be very handy in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyGrad's Math Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some familiarity with the mathematical functions supplied by mygrad, call `help` on: \n",
    " - [`mg.math`](https://mygrad.readthedocs.io/en/latest/math.html)\n",
    " - [`mg.math.arithmetic.funcs`](https://mygrad.readthedocs.io/en/latest/math.html#arithmetic-operations)\n",
    " - [`mg.math.sequential.funcs`](https://mygrad.readthedocs.io/en/latest/math.html#sums-products-differences)\n",
    " - [`mg.linalg.funcs`](https://mygrad.readthedocs.io/en/latest/linalg.html)\n",
    " \n",
    "All of these functions contained in these modules have a `backward` method, meaning that you can readily compute derivatives of them with respect to their inputs. More on this later.\n",
    "\n",
    "Take some time to do some simple computations with mygrad's math functions. Note that they are all available at the top-level of mygrad (`mg.<TAB>` to reveal the functions available). If you are comfortable with calculus, you can try checking some derivatives out. For example:\n",
    "\n",
    "```python\n",
    ">>> x = mg.Tensor(0)\n",
    ">>> mg.cos(x).backward() # computes the derivative d(cos(x))/dx\n",
    ">>> x.grad # stores df/dx @ x = 0\n",
    "array(-1.2246468e-16)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
