{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climbing Down Mountains with MyGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "import noggin as nog\n",
    "\n",
    "%matplotlib notebook\n",
    "# you may need to run this cell twice for the matplotlib notebook magic to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using mygrad's auto-differentiation functionality to perform gradient descent in various scenarios.\n",
    "\n",
    "This notebook will also introduce you to [noggin](https://noggin.readthedocs.io/en/latest/), which will permit us to conveniently record and plot our data in real-time! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a gradient-descent function\n",
    "Write a gradient descent function that takes in a tensor (`param`) and a *learning rate* ($\\delta$). \n",
    "\n",
    "This function assumes that you already have created some computational graph, and that you have already used back-propagation to compute the relavent derivatives. That is, `param.grad` stores the derivatives that you want to use to update `param` itself via gradient-descent.\n",
    "\n",
    "For the tensor, update its *underlying numpy array* according to the gradient descent. Use the augmented-update operator `-=` to update the array in-place (Why is this desirable? Chat with a neighbor). Because you are modifying the data of these tensors in-place, this function need not return anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(param, learning_rate):\n",
    "    \"\"\" Update tensors according to vanilla gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    param : mygrad.Tensor\n",
    "        The tensor to be updated using gradient-descent\n",
    "    learning_rate : float\n",
    "        The learning rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The parameter should be updated in-place\"\"\"\n",
    "    # <COGINST>        \n",
    "    if param.grad is not None:\n",
    "        param.data -= learning_rate * param.grad\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our discussion of derivatives, test your function. Define the computational graph for $f(x) = 2x + 1$, evaluated at $x=10$. Use `mygrad` to compute $\\frac{df}{dx}\\Bigr\\rvert_{x = 10.0}$ (which, of course, will be stored in `x.grad`)\n",
    "\n",
    "Pick a learning rate, and perform *one* gradient-descent step to update $x$. Before running the calculation write down what you expect the updated value of $x$ to be, and use this to test your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<COGINST>As we saw in the earlier notebook, $\\frac{df}{dx}\\Bigr\\rvert_{x = 10.0}$ is $2$. Thus our update should look like:\n",
    "\n",
    "\\begin{align}\n",
    "x_\\text{new} = x_\\text{old} - \\delta \\frac{df}{dx}\\Bigr\\rvert_{x = 10.0} = 10. - \\delta \\times 2\n",
    "\\end{align}\n",
    "\n",
    "Say we take $\\delta=3$, then our updated $x$ will be 4.\n",
    "</COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x = mg.Tensor(10.)\n",
    "f = 2*x + 1\n",
    "f.backward()\n",
    "grad_descent(x, learning_rate=3.)\n",
    "print(x)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending $f(x) = x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "# `plotter` is an object produced by noggin; it can be used to \n",
    "# record and plot our value of `x` as we update it\n",
    "\n",
    "# read the docstring for `create_plot`. What does `max_fraction_spent_plotting` do?\n",
    "plotter, fig, ax = nog.create_plot(\"x^2\", max_fraction_spent_plotting=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use gradient-descent to find the minimum value of $f(x) = x^2$ (which we should find to be $x=0$).\n",
    "\n",
    "Let's pick a starting value of $x=5.$ and a learning-rate of $\\delta=0.1$. We will need to compute derivatives with respect to $x$, so $x$ will need to be a mygrad-tensor. Define these values here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.max_fraction_spent_plotting = .7  # you can reduce this value to speed-up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "lr = 1E-2\n",
    "x = mg.Tensor(5.)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will make successive gradient-based steps (gradient and derivative mean the same thing here) to move $x$ towards the minimum. Define `num_updates` to store the number of steps successive steps that we want to take. Start with $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define `num_steps`\n",
    "# once you run this cell, take a look at the plot we set up previously\n",
    "# <COGINST>\n",
    "num_steps = 100\n",
    "# </COGINST>\n",
    "\n",
    "for cnt in range(num_steps):\n",
    "    # Evaluate f for the current value of x (i.e. perform a \"forward-pass\")\n",
    "    # <COGINST>\n",
    "    f = x**2\n",
    "    # </COGINST>\n",
    "    \n",
    "    # Use mygrad to compute df/dx\n",
    "    # <COGINST>\n",
    "    f.backward()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # Update `x` using gradient descent so that `f` will be smaller\n",
    "    # <COGINST>\n",
    "    grad_descent(x, lr)\n",
    "    # </COGINST>\n",
    "    \n",
    "    # We're finishes with our gradients, be sure to \"null\" them!\n",
    "    # <COGINST>\n",
    "    f.null_gradients()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # this will log our current value of `x` with noggin and plot it\n",
    "    plotter.set_train_batch({\"x^2\": x.item()}, batch_size=1)\n",
    "\n",
    "# This ensures that we have plotted our most up-to-date data\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we take enough steps to minimize $f$? Adjust `num_steps` and, if you'd like, the learning rate, and resume the descent until you near the actual minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending Multiple Independent Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we were able to find the minimum of $f(x)=x^2$. But what if we wanted to minimize multiple functions simultaneously? Thankfully `mygrad` has the machinery that will allow us to do so! Let's say we want to minimize the functions $f(x)=x$, $g(x)=x^2$, and $h(x)=x^3$. To do so, we can simply create a tensor that tracks the values of each of the three functions as we perform gradient descent. So, we can define a tensor that stores the values of $x$, then apply our functions to yield a tensor storing $\\begin{bmatrix}x & x^2 & x^3\\end{bmatrix}$.\n",
    "\n",
    "Instantiate a tensor below, where the starting value for each function is $x=7.$ (a float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x = mg.Tensor([7., 7., 7.])\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a `noggin` plot that will allow us to track the values of each function as we update with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "\n",
    "# read the docstring for `create_plot` to see the different valid data types for our plotting metrics\n",
    "plotter, fig, ax = nog.create_plot([\"x\", \"x^2\", \"x^3\"], max_fraction_spent_plotting=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how can we actually minimize our functions? Well, if we apply each of our functions to the elements in our tensor, when we call `backward`, the `grad` attribute will store the corresponding derivative for each function. Thus, after the first call of `backward`, `grad` will be of the form: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\\frac{d(x)}{dx}\\Bigr|_{x=10} & \\frac{d(x^2)}{dx}\\Bigr|_{x=10} & \\frac{d(x^3)}{dx}\\Bigr|_{x=10}\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Just as before, we will make repeated gradient descent steps to try to minimize our functions. But now, instead of our simply calling our function as `f = x ** 2`, we need to raise each element to a different power. Given that `mygrad` mirrors numpy's vectorization, is there a quick way we can do this (that doesn't require us to manually raise each element to a power)? \n",
    "\n",
    "For hyper-parameters, try using `num_steps` of $300$ and `lr` of $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define `num_steps` and `lr`\n",
    "# <COGINST>\n",
    "import numpy as np\n",
    "lr = 1E-2\n",
    "num_steps = 300\n",
    "# </COGINST>\n",
    "\n",
    "for cnt in range(num_steps):\n",
    "    # Evaluate f for the current value of x (i.e. perform a \"forward-pass\")\n",
    "    # <COGINST>\n",
    "    f = x ** np.array([1, 2, 3])\n",
    "    # </COGINST>\n",
    "    \n",
    "    # Use mygrad to compute df/dx\n",
    "    # <COGINST>\n",
    "    f.backward()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # Update `x` using gradient descent so that `f` will be smaller\n",
    "    # <COGINST>\n",
    "    grad_descent(x, lr)\n",
    "    # </COGINST>\n",
    "    \n",
    "    # We're finishes with our gradients, be sure to \"null\" them!\n",
    "    # <COGINST>\n",
    "    f.null_gradients()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # this will log our current value of `x` for each of our functions and plot it\n",
    "    plotter.set_train_batch({\"x\": x[0].item(), \n",
    "                             \"x^2\": x[1].item(),\n",
    "                             \"x^3\": x[2].item()}, batch_size=1)\n",
    "\n",
    "# This ensures that we have plotted our most up-to-date data\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see in the plots? Is this what we should expect? Discuss with a neighbor.\n",
    "\n",
    "If we wanted to, we could minimize functions other than $x$, $x^2$, and $x^3$. Take a look at the [math functions](https://mygrad.readthedocs.io/en/latest/math.html) offered by `mygrad`, and try minimizing a few. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending a Multivariate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we've only tried minimizing functions of one variable, i.e. of the form $f(x)$. But when we start training models with gradient descent, we'll need to minimize functions of hundreds or thousands of variables. How do we do this? Let's start with a simple example: $f(x,y)=x^2+y^2$. This can be visualized as:\n",
    "\n",
    "![multi](pics/multivar_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is a minimum at $x=y=0$. But how can gradient descent find this minimum for us? All we have to do is track the values for $x$ and $y$ in tensors, and once we call `f.backward()`, the appropriate gradients will accumulate within the `grad` attributes of the tensors. Below, define `x` and `y` tensors, with starting values of $x=3.7$ and $y=6.3$. Use a single tensor of the form $\\begin{bmatrix}x & y\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "v = mg.Tensor([3.7, 6.3])\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our `noggin` plot, which will track the values of both $x$ and $y$ as we perform our gradient descent steps. What should the plot for $x$ look like? For $y$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "\n",
    "# what will the `x` and `y` plots look like?\n",
    "plotter, fig, ax = nog.create_plot([\"x\", \"y\"], max_fraction_spent_plotting=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, perform the gradient descent updates to minimize our two variables. You can again use `num_steps` of $300$ and `lr` of $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define `num_steps` and `lr`\n",
    "# <COGINST>\n",
    "lr = 1E-2\n",
    "num_steps = 300\n",
    "# </COGINST>\n",
    "\n",
    "for cnt in range(num_steps):\n",
    "    # Evaluate f for the current value of x (i.e. perform a \"forward-pass\")\n",
    "    # <COGINST>\n",
    "    f = (v**2).sum()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # Use mygrad to compute df/dx\n",
    "    # <COGINST>\n",
    "    f.backward()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # Update both `x` and `y` using gradient descent so that `f` will be smaller\n",
    "    # <COGINST>\n",
    "    grad_descent(v, lr)\n",
    "    # </COGINST>\n",
    "    \n",
    "    # We're finishes with our gradients, be sure to \"null\" them!\n",
    "    # <COGINST>\n",
    "    f.null_gradients()\n",
    "    # </COGINST>\n",
    "    \n",
    "    # this will log our current value of `x` for each of our functions and plot it\n",
    "    plotter.set_train_batch({\"x\": v[0].item(), \n",
    "                             \"y\": v[1].item()}, batch_size=1)\n",
    "\n",
    "# This ensures that we have plotted our most up-to-date data\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try messing around with different functions of $x$ and $y$ or changing the learning rate hyperparameter. How does this change the behaviour of $x$ and $y$ as you perform gradient descent?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
