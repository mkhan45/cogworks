{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MyGrad \n",
    "\n",
    "## Installation Steps\n",
    "To proceed with this notebook, make sure that you have installed [`mygrad`](https://mygrad.readthedocs.io/en/latest/index.html) and [`noggin`](https://noggin.readthedocs.io/en/latest/):\n",
    "\n",
    "```shell\n",
    "pip install mygrad noggin\n",
    "```\n",
    "\n",
    "To produce visualizations with computational graphs, install `graphviz`:\n",
    "\n",
    "```shell\n",
    "conda install python-graphviz\n",
    "```\n",
    "\n",
    "Make sure you restart the kernel for this notebook if you installed any of these.\n",
    "\n",
    "## Introducing MyGrad: Computing the Slope of a Simple Function\n",
    "\n",
    "[mygrad](https://mygrad.readthedocs.io/en/latest/index.html) is a so-called \"automatic differentiation\" (a.k.a \"autograd\") numerical library. This means that `mygrad` is able to calculate results from mathematical functions, and then evaluate the *derivatives* (slopes) of those functions. Note that `mygrad` cannot compute analytical derivatives (i.e. the function that describes the derivative at all points). `mygrad` can only find the derivative evaluated at specified values.\n",
    "\n",
    "Let's consider an exceedingly-simple function, $f(x) = 2x + 1$. This is a line with a slope of 2. Thus the derivative of this function should be $2$ for all values of $x$. I.e. $\\frac{df(x)}{dx} = 2$. Let's see that `mygrad` can produce this result.\n",
    "\n",
    "First we specify the point, $x$, at which we want to evaluate this function. Let's use $x = 5.0$, and compute $f(5.0)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# run me\n",
    "import mygrad as mg\n",
    "\n",
    "# computing: f = 2x + 1, at x = 5.0\n",
    "x = mg.Tensor(5.0)\n",
    "f = 2*x + 1\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing $f(5.0)$, `mygrad` constructed a computational graph that tracks all of the numerical inputs and mathematical operations that were responsible for producing this result. Let's visualize the computational graph that is associated with $f(5.0)$:\n",
    "\n",
    "![title](pics/mygrad_graph.png)\n",
    "\n",
    "Because `mygrad` stores this computational graph, it knows how $f$ *depends* on $x$. This permits `mygrad` to answer the question:\n",
    "\n",
    ">\"if I increase $x$ slightly (infinitesimally) above $5.0$, by what proportion will $f$ change?\n",
    "\n",
    "or, equivalently:\n",
    "\n",
    ">\"holding all other variables fixed, what is the slope of $f$ at $x=5$?\n",
    "\n",
    "also equivalently, and more concisely:\n",
    "\n",
    ">\"What is $\\frac{df}{dx}\\Bigr\\rvert_{x = 5.0}$?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computational graph is tied specifically to the the terminal node of the graph, `f`. Let's see that the \"creator\" of `f` is an instance of mygrad's `Add` class, as depicted above. Evaluate the attribute `f.creator` in the cell below (as it is just an attribute, and not a method, you don't need to \"call\" it - no parentheses needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "f.creator\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the derivative of $f$ with respect to $x$; in `mygrad` we do this by invoking `f.backward()`. This says: \n",
    ">\"Starting with `f` traverse (backwards) through the graph and compute all the derivatives of $f$ with respect to any tensor in the graph.\"\n",
    "\n",
    "The values of these derivatives will be stored in `<var>.grad`, where `<var>` is any tensor in the graph. Thus `x.grad` will store the value $\\frac{df}{dx}\\Bigr\\rvert_{x = 5.0}$. Remind yourself, what should this value be?\n",
    "\n",
    "Invoke `f.backward()` and then inspect `x.grad` (which, like `<var>.creator`, is an attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger back-propagation and check df/dx\n",
    "# <COGINST>\n",
    "f.backward()\n",
    "x.grad\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the above cell multiple times in sequence, you should see that `x.grad` grows and grows... this is *not* what we expect. Because our computational graph has remained the same, we expect our derivative should also remain the same. The accumulation that we see is actually a quirk of the mechanics of the back-propagation algorithm. This leads us to a very important practical note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A very import note: once you have invoked back-propagation and are done using the gradients of a computational graph, you should always \"null\" those gradients (i.e. set the `grad` attribute to `None` for all tensors in the computational graph)**.\n",
    "\n",
    "You can do this by calling `<var>.null_gradients()`, where `<var>` is the **terminal tensor of your graph**. Null the gradients of this computational graph, and verify that the gradients of `x` and `f` are indeed `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null the gradients in this graph\n",
    "# <COGINST>\n",
    "f.null_gradients()\n",
    "assert f.grad is None\n",
    "assert x.grad is None\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should $\\frac{df}{dx}\\Bigr\\rvert_{x = -10.0}$ be? Verify your intuition using `mygrad`, re-doing the steps from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing: f = 2x + 1, at x = -10.0 and checking df/dx\n",
    "# <COGINST>\n",
    "# We are working with a line of slope 2, so the derivative \n",
    "# is 2 regardless of the value of x\n",
    "x = mg.Tensor(-10.0)\n",
    "f = 2*x + 1\n",
    "f\n",
    "f.backward()\n",
    "x.grad\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more interesting observation: after invoking back-propagation, inspect what `f.grad` is. This represents $\\frac{df}{df}$, or: \n",
    "\n",
    ">\"if I increase $f$ slightly (infinitesimally) above its present value, by what proportion will $f$ change?\n",
    "\n",
    "Given this description of $\\frac{df}{df}$, does the value for `f.grad` that you see make sense? Chat with a neighbor about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "# `f.grad` should always be array(1.) - the derivative of any variable with respect \n",
    "# to itself is, by definition, 1.\n",
    "f.grad\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "We have been introduced to `mygrad`, an auto-differentiation library. We saw that this library allows us to perform numerical calculations, and it stores a so-called computational graph that describes that that calculation. This permits `mygrad` to compute the derivatives of the terminal variable in that graph with respect to all of the other variables in that graph. It does this using the process of \"back-propagation\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MyGrad's Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mygrad` has a [`Tensor`](https://mygrad.readthedocs.io/en/latest/tensor.html) object, which is nearly identical to NumPy's array; it:\n",
    "- stores data in an [N-dimensional array-like patterns](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/AccessingDataAlongMultipleDimensions.html)\n",
    "- supports both [basic and advanced indexing](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/BasicIndexing.html) \n",
    "- performs computations over arrays of numbers intuitively and efficiently, by leveraging [vectorization](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html)\n",
    "- supports numpy's semantics of [broadcasting](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html) operations between tensors of different shapes\n",
    "\n",
    "Then how do numpy and mygrad differ? Whereas numpy simply performs a computation as necessary and stores no information about which arrays participate in it, as we saw above, mygrad keeps track of the computational graph that its tensors participate in. This is what permits mygrad to perform auto-differentiation (via back-propagation), which is the central purpose of mygrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, work through [this section of PLYMI](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/IntroducingTheNDarray.html), but convert *all* of the numpy functions and objects to mygrad objects. E.g. instead of `import numpy as np`, write `import mygrad as mg`, and so on. \n",
    "\n",
    "You can use multiple cells for this to help organize your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> import numpy as np\n",
    "# >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# <COGINST>\n",
    "import mygrad as mg\n",
    "x = mg.Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # An ND-array belongs to the type `numpy.ndarray`\n",
    "# >>> type(x)\n",
    "# numpy.ndarray\n",
    "\n",
    "# >>> isinstance(x, np.ndarray)\n",
    "# True\n",
    "\n",
    "# <COGINST>\n",
    "print(type(x))\n",
    "\n",
    "print(isinstance(x, mg.Tensor))\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> x = x.reshape(3,3)\n",
    "# >>> x\n",
    "# array([[0, 1, 2],\n",
    "#        [3, 4, 5],\n",
    "#        [6, 7, 8]])\n",
    "\n",
    "# <COGINST>\n",
    "x = x.reshape(3, 3)\n",
    "x\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> np.power(x, 2)  # can also be calculated using the shorthand: x**2\n",
    "# array([[ 0,  1,  4],\n",
    "#        [ 9, 16, 25],\n",
    "#        [36, 49, 64]], dtype=int32)\n",
    "\n",
    "# <COGINST>\n",
    "mg.power(x, 2)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> np.mean(x, axis=1)\n",
    "# array([ 1.,  4.,  7.])\n",
    "\n",
    "# <COGINST>\n",
    "mg.mean(x, axis=1)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following cell, try using a numpy-array, `np.array([0., 1., 2.])`, as the exponential for the tensor `x`. It might be surprising that this works! What might the value be of using a numpy-array within a computational graph as opposed to a mygrad tensor? Talk to your neighbors about this and consult with an instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> x ** np.array([0., 1., 2.])\n",
    "# array([[  1.,   1.,   4.],\n",
    "#        [  1.,   4.,  25.],\n",
    "#        [  1.,   7.,  64.]])\n",
    "\n",
    "# <COGINST>\n",
    "import numpy as np\n",
    "x ** np.array([0., 1., 2.])\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One distinct difference between numpy and mygrad is that, whereas you can access individual numbers from a numpy-array:\n",
    "\n",
    "```python\n",
    ">>> x = np.array([1, 2, 3])\n",
    ">>> x[1]  # returns an integer\n",
    "2\n",
    "```\n",
    "\n",
    "indexing into a `Tensor` will *always* return a `Tensor`:\n",
    "\n",
    "```python\n",
    ">>> x = mg.Tensor([1, 2, 3])\n",
    ">>> x[1]  # returns a 0-dimensional Tensor\n",
    "Tensor(2)\n",
    "```\n",
    "\n",
    "This is because mygrad has to be able to track all of the elements in all of the Tensors to reliable calculate derivatives for the computational graph. If you want to access the number, you can call `.item()` on a 0-dimensional Tensor (or a 0-dimensional numpy array):\n",
    "\n",
    "```python\n",
    ">>> x[1].item()\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the [documentation for mygrad's Tensor](https://mygrad.readthedocs.io/en/latest/tensor.html). Among the other details provided there, take note: what does `Tensor.data` store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Many Derivatives at Once\n",
    "\n",
    "Compute $f(x) = 2x + 1$ on the domain $[-1, 1]$ sampling this domain evenly using $10$ points. Use a `mygrad` function instead of a numpy function (hint: many of the numpy creation functions such as `arange` and `linspace` are replicated in `mygrad`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x = mg.linspace(-1, 1, 10)\n",
    "f = 2*x + 1\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke back-propagation to compute the derivatives of $f$. What type of object is of `x.grad`? What is the shape of `x.grad`. Discuss with your neighbor the following questions: \n",
    "\n",
    "- what does each element of `x` represent?\n",
    "- what does each element of `f` represent?\n",
    "- what does each element of `x.grad` represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "    \n",
    "- `x` the ten samples in our domain. \n",
    "- `f` stores $2x + 1$ for each value of `x` \n",
    "- `x.grad`, then stores the derivative of $f$ evaluated at *each* location of `x`\n",
    "</COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot $f(x) = x^2$ on the domain $[-1, 1]$, sampling this domain evenly using $1,000$ points. In the same figure, plot $\\frac{df}{dx}$. \n",
    "\n",
    "Before you render your plot: what should the value of $\\frac{df}{dx}$ be at $x=0$ (what is the slope of $x^2$ at the origin?). \n",
    "\n",
    "What *sign* should $\\frac{df}{dx}$ have for $x < 0$? For $0 < x$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# <COGINST>\n",
    "x = mg.linspace(-1, 1, 1000)\n",
    "f = x**2\n",
    "f.backward()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f)\n",
    "ax.plot(x, x.grad)\n",
    "ax.grid()\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflecting on this plot, what is the slope of $f(x)$ at $x=-1$?. Does this plot reaffirm your interpretation of what `x.grad` represents for when `x` stores many numbers? Discuss with a neighbor. Flag an instructor if no one is quite sure.\n",
    "\n",
    "If you have not taken calculus before, can you deduce what the functional form is of $\\frac{df}{dx}$, for $f(x) = x^2$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "$\\frac{df}{dx}\\Bigr\\rvert_{x = -1.0} = -2$\n",
    "\n",
    "`x.grad` stores $\\frac{df}{dx}$ evaluated at ${x_0}, {x_1}, ..., {x_{N-1}}$\n",
    "\n",
    "As one can deduce from the plot, $\\frac{df}{dx} = 2x$\n",
    "</COGINST>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
