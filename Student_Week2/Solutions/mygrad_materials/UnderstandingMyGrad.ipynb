{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding MyGrad\n",
    "> CogWorks 2018 (Petar Griggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell!\n",
    "import mygrad as mg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a list of numbers ranging from 0 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(range(51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a [`Tensor`](https://mygrad.readthedocs.io/en/latest/tensor.html)-instance by passing it some iterable of numbers. This includes lists, tuples, or NumPy arrays. Any array-like sequence of numbers will suffice. When we do so, the `Tensor.__init__()` simply converts this iterable to a NumPy `ndarray`. We can inspect a Tensor's underlying NumPy array by calling `<tensor>.data`. \n",
    "\n",
    "The takeaway is that: whatever data you pass in when creating a tensor will be stored as a NumPy array. You can access that underlying data **but should never unwittingly modify that NumPy array directly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass `ls` to `mg.Tensor` and check the shape and contents of the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x = mg.Tensor(ls)\n",
    "x.shape\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `data` attribute of the Tensor that you created and see that it is the Tensor's underlying NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x.data\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try mutating the underlying NumPy array using an augmented assignment (e.g. `x += 2`), and see that this changes the Tensor correspondingly. This will come in handy when we are updating parameters via gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x.data += 2\n",
    "x\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try constructing Tensors with other iterables below:\n",
    " - a 3D NumPy array\n",
    " - a 2D structure of nested lists\n",
    " - a single number (the only permissable non-iterable object that can be passed to `Tensor.__init__`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x_3d = mg.Tensor(np.arange(27).reshape(3, 3, 3))\n",
    "x_2d = mg.Tensor([[0, 1], [2, 3]])\n",
    "x_0d = mg.Tensor(3)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyGrad also has many convenient tensor-creation functions that mimic those of NumPy, such as [`arange`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.arange.html#mygrad.arange) and [`linspace`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.linspace.html#mygrad.linspace).\n",
    "\n",
    "Execute `help(mg.tensor_creation.funcs)` to view these tensor-creation functions and their documentation, or see the [`mygrad` docs](https://mygrad.readthedocs.io/en/latest/tensor_creation.html). Notice that each of these functions accepts an optional `constant` argument. What do you suppose the purpose of this is?\n",
    "\n",
    "Use `mg.linspace` to create a constant `Tensor` consisting of 100 points sampled on $[-1, 1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "mg.linspace(-1, 1, 100, constant=True)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tensor Attributes\n",
    "Tensors also store a number of other attributes: `scalar_only`, `grad`, `creator`, and `constant`. You can inspect the Tensor docstring for a description of some of these or read the docs [here](https://mygrad.readthedocs.io/en/latest/tensor.html#documentation-for-mygrad-tensor), but we will also describe them more in detail in due time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we perform mathematical operations using Tensors, we build up a computational graph whose nodes are Tensors and Operations. The edges connect tensor-nodes with operation-nodes.\n",
    "\n",
    "Edges connect input-tensors to operations, and operations to output-tensors. Each new operation we perform adds new nodes and edges to our computational graph. Below is an example of a computational graph that you may remember from the online course, adapted slightly for MyGrad.\n",
    "\n",
    "The following series of operations:\n",
    "\n",
    "```python\n",
    "x = mg.Tensor(10)\n",
    "y = np.array(15)\n",
    "out1 = x + y\n",
    "z = 2\n",
    "out2 = out1 * z\n",
    "```\n",
    "\n",
    "creates the computational graph:\n",
    "\n",
    "![comp_graph](pics/comp_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The creator attribute\n",
    "Each tensor has a [`creator`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.Tensor.creator.html#mygrad.Tensor.creator) attribute. This stores a reference to the Operation-instance that created that Tensor. `creator` will be `None` for any Tensor that was initialized directly, and not produced by a mathematical operation.\n",
    "\n",
    "#### The variables attribute\n",
    "Similarly, every Operation has a `variables` attribute, a tuple which stores all of the Tensors that acted as its inputs.\n",
    "\n",
    "Together, `Tensor.creator` and `Operation.variables` are the two attributes required for defining the structure of any computational graph.\n",
    "\n",
    "#### Tracing through a computational graph\n",
    "Create the computational graph detailed above and inspect the creator-attribute of `out2`. Confirm that this is an instance of the `Multiplication` operation. Next, check the variables-attribute of that operation; it should store a tuple containing `out1` and `z`. Continue on until you've traced backward through the computational graph, back to `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x = mg.Tensor(10)\n",
    "y = np.array(15)\n",
    "out1 = x + y\n",
    "z = 2\n",
    "out2 = out1 * z\n",
    "print(out2.creator)\n",
    "print(out2.creator.variables)\n",
    "print(out1.creator)\n",
    "print(out1.creator.variables)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyGrad must construct this computational graph so that it may use back-propagation to compute derivatives. More on this later. In the meantime it is critical to know that MyGrad is constructing these computational graphs \"under the hood\" whenever we perform mathematical operations on its Tensors.\n",
    "\n",
    "You may be surprised to see that a NumPy-array was able to be used in your computational graph - MyGrad so closely mirrors NumPy that it is natural to permit the use of NumPy-arrays in its computational graphs. The key detail here is that *NumPy-arrays will always be treated as constants in computational graphs*. That is, if we perform back-propagation on this graph, no derivative will be computed for `y`, since it is a NumPy-array. This will prove to be very handy in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyGrad's Math Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some familiarity with the mathematical functions supplied by mygrad, call `help` on: \n",
    " - [`mg.math`](https://mygrad.readthedocs.io/en/latest/math.html)\n",
    " - [`mg.math.arithmetic.funcs`](https://mygrad.readthedocs.io/en/latest/math.html#arithmetic-operations)\n",
    " - [`mg.math.sequential.funcs`](https://mygrad.readthedocs.io/en/latest/math.html#sums-products-differences)\n",
    " - [`mg.linalg.funcs`](https://mygrad.readthedocs.io/en/latest/linalg.html)\n",
    " \n",
    "All of these functions contained in these modules have a `backward` method, meaning that you can readily compute derivatives of them with respect to their inputs. More on this later.\n",
    "\n",
    "Take some time to do some simple computations with mygrad's math functions. Note that they are all available at the top-level of mygrad (`mg.<TAB>` to reveal the functions available). If you are comfortable with calculus, you can try checking some derivatives out. For example:\n",
    "\n",
    "```python\n",
    ">>> x = mg.Tensor(0)\n",
    ">>> mg.cos(x).backward() # computes the derivative d(cos(x))/dx\n",
    ">>> x.grad # stores df/dx @ x = 0\n",
    "array(-1.2246468e-16)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
